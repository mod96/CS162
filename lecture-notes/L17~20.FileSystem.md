# 1. I/O

<p align="center">
    <img src="imgs/17_2.PNG" width="40%" />
    <img src="imgs/17_3.PNG" width="59%" />
</p>


## 1.1. CPU and Controller

### 1.1.1. PMIO, MMIO

<p align="center">
    <img src="imgs/17_4.PNG" width="50%" />
    <img src="imgs/17_5.PNG" width="15%" />
</p>

- CPU interacts with a Controller
  - Contains a set of registers that can be read and written
  - May contain memory for request queues, etc.
- Processor accesses registers in two ways:
  - **Port-Mapped I/O** (PMIO): in/out instructions
    - Often used in simpler or legacy systems. Speaker in pintos
    - CPU uses special I/O instructions (IN, OUT) to interact with the device controller through dedicated I/O ports.These ports are not part of the main memory address space but exist in a separate I/O address space. The request is routed to the appropriate device controller via the system bus.
    - Example from the Intel architecture: out 0x21,AL
  - **Memory-mapped I/O** (MMIO): load/store instructions
    - Common in modern systems and complex devices, like GPUs and high-speed network cards.
    - Registers/memory appear in physical address space. CPU reads from or writes to specific memory addresses that are mapped to the device registers. The memory controller interprets these requests as device register accesses and forwards them to the appropriate device via the system bus. The device controller writes back data to the mapped memory address, which the CPU reads during subsequent instructions.
    - I/O accomplished with load and store instructions
    - Safe by address translation

### 1.1.2. PIO, DMA

- **Programmed I/O (PIO)**:
  - In PIO, the CPU directly manages the data transfer between memory and the device. The CPU explicitly reads data from or writes data to the device controller, one operation at a time.
  - Flow:
    - Device Signals Readiness (Polling or Interrupt):
      - The device signals that it is ready to send/receive data:
        - Polling: The CPU continuously checks (polls) a status register on the device for readiness.
        - Interrupts: The device generates an interrupt when ready, reducing CPU idle time.
    - CPU Initiates Data Transfer:
      - For Input:
        - The CPU sends a read command to the device.
        - The device controller places the data on the data bus.
        - The CPU reads the data from the bus and stores it in a register or memory.
      - For Output:
        - The CPU writes data to a specific memory/register associated with the device.
        - The device controller retrieves this data from the bus.
    - CPU Repeats Process Until Done:
      - The CPU repeats the above steps for each piece of data.
      - This can be inefficient because the CPU is heavily involved in every step.
- **Direct Memory Access (DMA)**:
  - DMA is a more efficient method for data transfer, particularly for large volumes of data. A dedicated **DMA controller** manages the data transfer between memory and the device, freeing the CPU for other tasks.
  - Flow:
    - CPU Configures DMA Controller:
      - The CPU initializes the DMA controller with the following parameters:
        - Source address (where the data resides, e.g., memory or device buffer).
        - Destination address (where the data needs to go).
        - Size of the data to be transferred.
        - Transfer direction (memory-to-device or device-to-memory).
      - The CPU signals the DMA controller to start the transfer.
    - DMA Controller Takes Over:
      - The DMA controller takes control of the system bus (via bus arbitration) to perform the data transfer.
      - For Input (Device-to-Memory):
        - The DMA controller reads data from the device buffer.
        - It writes the data directly into system memory.
      - For Output (Memory-to-Device):
        - The DMA controller reads data from system memory.
        - It writes the data directly into the device buffer.
    - Device Notifies Completion:
      - Once the DMA transfer is complete, the device signals the CPU (usually via an interrupt).
    - CPU Resumes Work:
      - The CPU is interrupted, and it processes the notification of completion.
      - Any additional operations (e.g., checking data integrity, post-transfer processing) can be performed.

<p align="center">
    <img src="imgs/17_6.PNG" width="49%" />
    <img src="imgs/17_7.PNG" width="49%" />
</p>

### 1.1.3. I/O Device Notifying the OS

- I/O Interrupt
  - Devices use interrupts to notify the CPU that an event has occurred (e.g., data is ready, an operation is complete).
  - **Interrupt Controller**:
    - The device raises an interrupt signal to the CPU.
    - The interrupt controller (e.g., APIC, PIC) prioritizes and routes the interrupt to the CPU.
  - Interrupt Handling:
    - The CPU halts its current execution and invokes an interrupt service routine (ISR), a special function designed to handle the event.
    - The ISR interacts with the device, often using PMIO or MMIO, to retrieve data or handle the event.
  - Pro: handles unpredictable events well
  - Con: interrupts relatively high overhead
- Polling:
  - OS periodically checks a device-specific status register
    - I/O device puts completion information in status register
  - Pro: low overhead
  - Con: may waste many cycles on polling if infrequent or unpredictable I/O operations

The hybrid approach of combining interrupts and polling is commonly employed to optimize system performance by balancing the trade-offs of each method. Interrupts are efficient for low-traffic scenarios as they notify the CPU as soon as data is ready, reducing latency. However, frequent interrupts can overwhelm the CPU, particularly in high-throughput systems like network adapters, where generating an interrupt for every packet would leave the CPU handling interrupts rather than processing data. On the other hand, polling minimizes interrupt overhead by having the CPU repeatedly check the device’s status, but this wastes CPU cycles when there is little or no data to process. The hybrid approach seeks to leverage the strengths of both methods. For instance, in a network adapter, an interrupt is used to notify the CPU when the first packet in a burst arrives, alerting the system to process it. The system then switches to polling to efficiently handle the rest of the packets in the queue, avoiding the overhead of frequent interrupts. Once the queue is empty or a predefined time elapses, the system reverts to waiting for interrupts for new incoming packets.

This hybrid mechanism is widely used in various high-performance scenarios. In storage systems, for example, an interrupt might signal the CPU that a disk operation is complete, after which polling can handle the transfer of subsequent data blocks, avoiding the cost of multiple interrupts for a large file transfer. Similarly, in USB devices, interrupts are used for infrequent data transfers, such as mouse or keyboard inputs, while bulk transfers—like those from USB storage or cameras—may rely on polling to improve throughput. Graphics processing units (GPUs) also benefit from this approach, where interrupts notify the CPU of task completion, and polling is used to manage the pipeline of subsequent rendering operations. Another notable application is in high-performance networking, such as in the Data Plane Development Kit (DPDK), where polling is primarily used to handle packets for maximum throughput, while interrupts manage control events like link status changes.

## 1.2. HDD, SSD

### 1.2.1. HDD

<p align="center">
    <img src="imgs/18_1.PNG" width="50%" />
</p>

For example, 7200RPM HDD, avg seek time 5ms, time for rotation is:

$$60000 (ms/min) / 7200(rev/min) = 8.3(ms/rev)$$

So avg rotation delay is about 4ms. (half rotation)

Transfer rate of $50MB/s$, sector size(block size) $4KB$ have

$$4096(B)/(50×10^6(B/s)) = 0.082 (ms/sector)$$

- Reading block from random place on disk has:
  - seek time 5ms + rotation delay 4ms + transfer time 0.082ms = 9.082ms
  - read throughput is $4096B/9.082ms=451KB/s$
- Read block from random place in same cylinder:
  - rotation delay 4ms + transfer time 0.082ms = 4.082ms
  - read throughput is $4096B/4.082ms=1.03MB/s$
- Read next block on same track:
  - read throughput is $4096B/0.082ms=50MB/s$

So, key to using disk effectively (especially for file systems) is to minimize seek and
rotational delays.

### 1.2.2. SSD

<p align="center">
    <img src="imgs/18_2.png" width="60%" />
    <img src="imgs/18_3.png" width="30%" />
</p>

NAND flash cannot overwrite existing data directly because of how it stores information as electrical charges within cells. Instead, the data must first be erased before new data can be written. This is tied to the way NAND memory is structured: data is organized into pages (the smallest writable unit, typically 4KB-16KB), while erasure occurs in larger units called blocks (typically 128KB-4MB). If a page within a block already contains data, the entire block must be erased before the page can be rewritten.

This limitation is rooted in the physical constraints of NAND cells. Data is stored in floating gates as electrical charges, and overwriting these charges directly is not feasible because of interference between adjacent cells. Erasing a block resets all the cells within it, removing the stored charges and preparing them to accept new data. Although this erasure-before-write process is fundamental to NAND flash memory, modern SSDs employ a variety of techniques to mitigate its impact on performance.

**Flash Translation Layer (FTL)** and **Copy-on-Write (CoW)** are two key techniques used in managing SSDs to optimize performance, reliability, and longevity. Together, these mechanisms address the unique characteristics and limitations of NAND flash memory, enabling faster and more efficient operation.

**Flash Translation Layer (FTL)** is a critical abstraction layer in SSDs that maps logical block addresses (LBAs) from the operating system to physical addresses on the NAND flash memory. This mapping is necessary because NAND flash memory cannot directly overwrite data due to its erase-before-write limitation. FTL manages this complexity efficiently, allowing the SSD to present itself to the operating system like a traditional block storage device while internally optimizing data placement and retrieval.

- Wear Leveling:
  - NAND cells have a finite number of program/erase (P/E) cycles. FTL ensures that write operations are evenly distributed across the memory to prevent some cells from wearing out prematurely. This increases the lifespan of the SSD while avoiding performance bottlenecks from overused cells.
- Garbage Collection:
  - FTL handles invalidated data by consolidating valid data from partially used blocks into new blocks, freeing up space for future writes. This process minimizes the number of erase cycles needed, ensuring smoother and faster writes over time.
- Write Amplification Reduction:
  - By intelligently managing how data is written and erased, FTL minimizes write amplification—a phenomenon where the amount of actual data written to NAND is greater than the data intended to be written. Lower write amplification results in faster performance and longer SSD lifespan.
- Mapping Tables:
  - FTL uses mapping tables to redirect writes to free pages instead of overwriting existing data. This redirection eliminates the need to erase blocks immediately and allows SSDs to write new data faster, as the erasure can be deferred to a background process.

**Copy-on-Write (CoW)** is a data management technique that avoids overwriting existing data. Instead of modifying data in place, CoW creates a new copy of the data with the changes applied, leaving the original data intact. Once the new data is successfully written, metadata is updated to point to the new version, and the old version may eventually be discarded during garbage collection.

- Efficient Writes:
  - By writing new data to free pages instead of erasing and rewriting old data, CoW eliminates the overhead associated with the erase-before-write process. This enables faster write operations, particularly for workloads with frequent updates.
- Crash Consistency:
  - CoW ensures data integrity during power failures or crashes. Since the original data remains unmodified until the new data is fully written, there is no risk of corruption from incomplete writes. This is particularly important for databases and filesystems.
- Better Utilization of Parallelism:
  - Modern SSDs can perform multiple write operations in parallel across different NAND dies. CoW leverages this capability by writing to available free pages simultaneously, improving throughput for workloads with high write demands.
- Simplified Garbage Collection:
  - CoW naturally aligns with garbage collection by creating new versions of data, leaving old data to be invalidated and eventually cleaned up. This reduces the complexity of managing data blocks and improves garbage collection efficiency.

## 1.3. Queueing Theory

- Latency $L$
- Request Rate $\lambda$
- Service Rate $\mu$
- Utilization $\rho=\frac{\lambda}{\mu_{max}}$
- Queueing deley $d$
- Operation time $t$

### 1.3.1. Simple, Deterministic World

Events arrive exactly after $T_A$ time.

<p align="center">
    <img src="imgs/19_1.png" width="60%" />
</p>


<p align="center">
    <img src="imgs/19_2.png" width="50%" />
</p>


### 1.3.2. Random Events

Events arrive with average rate $1/T_A$ but is burstable. Every events are independent to each other. This is poisson point process.

<p align="center">
    <img src="imgs/19_3.png" width="60%" />
</p>

<p align="center">
    <img src="imgs/19_4.png" width="50%" />
</p>

Using Little's Law,

$L_Q = \lambda T_Q = \frac{\rho}{T_S} T_Q = \frac{\rho^2}{1-\rho}$,

where $L_Q$ is the average length of the queue.

So if utilization goes to 1, i.e. arrival rate becomes similar to service rate, the average length of the queue becomes infinite, resulting queueing time becoming infinite.

<p align="center">
    <img src="imgs/19_5.png" width="60%" />
</p>

### 1.3.3. We need to make disk performance more faster

To enhance disk performance, it is crucial to optimize for specific workload patterns and leverage system-level strategies effectively. For instance, performance can be improved during **large sequential reads** by optimizing access patterns to minimize seek times and maximize throughput. Similarly, when the workload is substantial, techniques like **reordering queues** allow multiple tasks to be executed efficiently by piggybacking operations, reducing the overall I/O latency. *During periods of low activity, slight inefficiencies may be tolerable*, as the system can prioritize responsiveness over optimization. Bursts of activity, however, present both a challenge and an opportunity—by strategically handling these bursts, such as caching frequently accessed data or prefetching likely-needed resources, the system can mitigate delays. One potential optimization could involve trading storage space for speed, such as using larger blocks or dedicating space for faster access methods. Other advanced techniques include reducing overhead with user-level drivers to bypass kernel bottlenecks and overlapping I/O operations with other productive tasks to make better use of idle CPU cycles. Together, these strategies aim to balance speed, efficiency, and resource utilization.

#### reordering queues

<p align="center">
    <img src="imgs/19_6.png" width="60%" />
    <img src="imgs/19_7.png" width="60%" />
</p>

# 2. FileSystem

A file system is a critical layer of the operating system designed to abstract and enhance the raw block interface of storage devices, such as disks, into a user-friendly structure of files and directories. At its core, the file system manages the mapping of human-readable file names to blocks of data on the disk, enabling users to interact with their data efficiently and intuitively. Key functionalities of a file system include **naming**, which allows users to locate files by their names rather than block numbers, and **organization**, which ensures that files are properly mapped to disk blocks. It also provides **protection** mechanisms to enforce access controls and **reliability** features to safeguard data against crashes and hardware failures, ensuring durability and accessibility of stored information.

<p align="center">
    <img src="imgs/20_1.png" width="50%" />
</p>

From a user’s perspective, the file system represents durable data structures that store information persistently, regardless of system reboots or interruptions. However, from the operating system’s viewpoint, it manages a collection of bytes, treating files as a sequential arrangement of these bytes without concern for their internal structure. Internally, the file system deals with collections of blocks, where a **block** is the logical transfer unit, often larger than the physical sector size of the disk. For example, in UNIX systems, blocks are typically 4KB in size. This layered abstraction separates user-level operations from the underlying physical management, enabling diverse applications to store and retrieve complex data structures seamlessly.


Disk management is a foundational aspect of the file system. The disk itself is viewed as a linear array of **sectors**, which can be identified using logical block addressing (LBA). Each sector has a unique integer address, and the disk controller translates these addresses into their corresponding physical positions, abstracting the complexity of the disk’s physical geometry. The file system must maintain various metadata structures to track free disk blocks, map file data to blocks, and organize files within directories. These responsibilities ensure efficient space utilization, quick access to files, and robust file organization.

To achieve its objectives, the file system relies on specific data structures stored on the disk. Unlike memory-resident structures, **disk-based data structures must be optimized for block-based access**, as individual words cannot be read or written efficiently. Sequential access patterns are preferred to minimize latency and maximize throughput. **Durability** is another critical design goal, ensuring that the file system remains in a consistent state even after unexpected shutdowns or crashes. Though achieving perfect durability is challenging, modern file systems incorporate techniques like **journaling** and **transactional updates** to minimize the risk of data corruption and maintain operational integrity.

<p align="center">
    <img src="imgs/20_2.png" width="45%" />
</p>

When a program executes an `open(file)` system call, the file system begins a process known as **name resolution** to translate the provided file path into a file number or inode, which serves as a unique identifier for the file. This process involves traversing the directory structure, which is itself a file containing mappings of `<file_name, file_number>` pairs, known as directory entries. Starting from the root directory or the current working directory (for relative paths), the system sequentially reads the directory entries at each level of the path hierarchy until it locates the target file. For example, resolving `/my/book/count` involves reading the root directory’s data to locate `my`, then reading the `my` directory to find `book`, and so on until the final file, `count`, is located.

Once the file is located, the file system retrieves its **inode** from the disk. The inode contains metadata about the file, including its type, size, permissions, and pointers to the blocks where the file’s data resides. This inode is then loaded into an in-memory data structure, the system-wide open file table, which maintains an entry for every opened file. Notably, the table contains only one entry per file, regardless of how many processes have opened it, ensuring efficient management of shared resources. The `open()` call returns a file handle to the program, which is essentially an index to this table, allowing subsequent `read()` or `write()` operations to interact directly with the in-memory inode and bypass the need for repeated disk lookups.

<p align="center">
    <img src="imgs/20_3.png" width="60%" />
</p>

The process of directory traversal and inode loading is designed to be efficient but can involve multiple disk accesses, especially for deep directory hierarchies. To mitigate this, modern file systems often cache directory structures and inodes in memory, significantly reducing the need for disk I/O during repeated operations. Additionally, directories are treated as specialized files, and access to their raw bytes is restricted by the operating system. Instead, functions like `readdir()` provide controlled access to directory contents, ensuring the integrity of the file system structure and preventing unauthorized modifications to directory data. This abstraction maintains a clear boundary between user processes and the internal mechanics of the file system, enhancing both reliability and security.

## 2.1. FAT: File Allocation Table

File Allocation Table (FAT) is a file system developed for personal computers and was the default filesystem for the MS-DOS and Windows 9x operating systems. Originally developed in 1977 for use on floppy disks, it was adapted for use on hard disks and other devices. The increase in disk drive capacity over time drove modifications to the design that resulted in versions: FAT12, FAT16, FAT32, and exFAT. FAT was replaced with NTFS as the default file system on Microsoft operating systems starting with Windows XP. Nevertheless, FAT continues to be commonly used on relatively small capacity solid-state storage technologies such as SD card, MultiMediaCard (MMC) and eMMC because of its compatibility and ease of implementation.

<p align="center">
    <img src="imgs/20_4.png" width="30%" />
</p>

In the FAT file system, reading and writing operations involve interacting with both the directory structure and the File Allocation Table (FAT) to manage file metadata and block mappings.

#### Example: Reading from File 31, Block 2, Offset X

To read from file 31, block 2, offset x, the process begins by locating the file in the directory. Starting at the root directory, the system searches for the directory entry associated with file number 31. Once found, the FAT is indexed using the file number, following its linked list of entries to identify the location of block 2. The system retrieves the block from the disk into memory, then accesses the data at the specified offset x.

#### Example: Writing to File 31, Block 3, Offset Y

For a write operation to file 31, block 3, offset y, the process starts similarly by locating file 31 in the directory and indexing into the FAT using the file number. The FAT is then used to navigate to the chain of blocks associated with the file. If block 3 is not yet allocated, a free block is selected by finding a FAT entry marked as "free." This free block's FAT entry is updated to link to the existing chain of file 31, making it the new block 3. The data for file 31, block 3 is then written to the corresponding block on disk, and the FAT entry for block 3 is updated to reflect the new allocation. This ensures the file's structure remains intact and accessible for future operations.

<p align="center">
    <img src="imgs/20_5.png" width="50%" />
</p>

Directory is implemented as a special file containing a linked list of entries, where each entry stores `<file_name: file_number>` mappings along with file attributes, such as permissions, timestamps, and size. Unlike modern file systems, **FAT stores file attributes directly within the directory entries** rather than associating them with the file's metadata elsewhere. This structure requires a linear search to locate specific entries, making directory access less efficient for large directories. The root directory ("/") is located at a fixed position on the disk, typically starting at block 2, as blocks 0 and 1 are reserved. Additional directories are represented as files themselves, extending the linked list structure, and allowing hierarchical organization within the FAT file system.

i.e. In the original design of the FAT (File Allocation Table) file system, every FAT entry is part of a single linked list structure that spans the entire disk starting from root.

## 2.2. UFS: Unix File System (Berkeley FFS)



## 2.3. Windows NTFS: New Technology File System



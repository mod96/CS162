# 1. I/O

<p align="center">
    <img src="imgs/17_2.PNG" width="40%" />
    <img src="imgs/17_3.PNG" width="59%" />
</p>


## 1.1. CPU and Controller

### 1.1.1. PMIO, MMIO

<p align="center">
    <img src="imgs/17_4.PNG" width="50%" />
    <img src="imgs/17_5.PNG" width="15%" />
</p>

- CPU interacts with a Controller
  - Contains a set of registers that can be read and written
  - May contain memory for request queues, etc.
- Processor accesses registers in two ways:
  - **Port-Mapped I/O** (PMIO): in/out instructions
    - Often used in simpler or legacy systems. Speaker in pintos
    - CPU uses special I/O instructions (IN, OUT) to interact with the device controller through dedicated I/O ports.These ports are not part of the main memory address space but exist in a separate I/O address space. The request is routed to the appropriate device controller via the system bus.
    - Example from the Intel architecture: out 0x21,AL
  - **Memory-mapped I/O** (MMIO): load/store instructions
    - Common in modern systems and complex devices, like GPUs and high-speed network cards.
    - Registers/memory appear in physical address space. CPU reads from or writes to specific memory addresses that are mapped to the device registers. The memory controller interprets these requests as device register accesses and forwards them to the appropriate device via the system bus. The device controller writes back data to the mapped memory address, which the CPU reads during subsequent instructions.
    - I/O accomplished with load and store instructions
    - Safe by address translation

### 1.1.2. PIO, DMA

- **Programmed I/O (PIO)**:
  - In PIO, the CPU directly manages the data transfer between memory and the device. The CPU explicitly reads data from or writes data to the device controller, one operation at a time.
  - Flow:
    - Device Signals Readiness (Polling or Interrupt):
      - The device signals that it is ready to send/receive data:
        - Polling: The CPU continuously checks (polls) a status register on the device for readiness.
        - Interrupts: The device generates an interrupt when ready, reducing CPU idle time.
    - CPU Initiates Data Transfer:
      - For Input:
        - The CPU sends a read command to the device.
        - The device controller places the data on the data bus.
        - The CPU reads the data from the bus and stores it in a register or memory.
      - For Output:
        - The CPU writes data to a specific memory/register associated with the device.
        - The device controller retrieves this data from the bus.
    - CPU Repeats Process Until Done:
      - The CPU repeats the above steps for each piece of data.
      - This can be inefficient because the CPU is heavily involved in every step.
- **Direct Memory Access (DMA)**:
  - DMA is a more efficient method for data transfer, particularly for large volumes of data. A dedicated **DMA controller** manages the data transfer between memory and the device, freeing the CPU for other tasks.
  - Flow:
    - CPU Configures DMA Controller:
      - The CPU initializes the DMA controller with the following parameters:
        - Source address (where the data resides, e.g., memory or device buffer).
        - Destination address (where the data needs to go).
        - Size of the data to be transferred.
        - Transfer direction (memory-to-device or device-to-memory).
      - The CPU signals the DMA controller to start the transfer.
    - DMA Controller Takes Over:
      - The DMA controller takes control of the system bus (via bus arbitration) to perform the data transfer.
      - For Input (Device-to-Memory):
        - The DMA controller reads data from the device buffer.
        - It writes the data directly into system memory.
      - For Output (Memory-to-Device):
        - The DMA controller reads data from system memory.
        - It writes the data directly into the device buffer.
    - Device Notifies Completion:
      - Once the DMA transfer is complete, the device signals the CPU (usually via an interrupt).
    - CPU Resumes Work:
      - The CPU is interrupted, and it processes the notification of completion.
      - Any additional operations (e.g., checking data integrity, post-transfer processing) can be performed.

<p align="center">
    <img src="imgs/17_6.PNG" width="49%" />
    <img src="imgs/17_7.PNG" width="49%" />
</p>

### 1.1.3. I/O Device Notifying the OS

- I/O Interrupt
  - Devices use interrupts to notify the CPU that an event has occurred (e.g., data is ready, an operation is complete).
  - **Interrupt Controller**:
    - The device raises an interrupt signal to the CPU.
    - The interrupt controller (e.g., APIC, PIC) prioritizes and routes the interrupt to the CPU.
  - Interrupt Handling:
    - The CPU halts its current execution and invokes an interrupt service routine (ISR), a special function designed to handle the event.
    - The ISR interacts with the device, often using PMIO or MMIO, to retrieve data or handle the event.
  - Pro: handles unpredictable events well
  - Con: interrupts relatively high overhead
- Polling:
  - OS periodically checks a device-specific status register
    - I/O device puts completion information in status register
  - Pro: low overhead
  - Con: may waste many cycles on polling if infrequent or unpredictable I/O operations

The hybrid approach of combining interrupts and polling is commonly employed to optimize system performance by balancing the trade-offs of each method. Interrupts are efficient for low-traffic scenarios as they notify the CPU as soon as data is ready, reducing latency. However, frequent interrupts can overwhelm the CPU, particularly in high-throughput systems like network adapters, where generating an interrupt for every packet would leave the CPU handling interrupts rather than processing data. On the other hand, polling minimizes interrupt overhead by having the CPU repeatedly check the device’s status, but this wastes CPU cycles when there is little or no data to process. The hybrid approach seeks to leverage the strengths of both methods. For instance, in a network adapter, an interrupt is used to notify the CPU when the first packet in a burst arrives, alerting the system to process it. The system then switches to polling to efficiently handle the rest of the packets in the queue, avoiding the overhead of frequent interrupts. Once the queue is empty or a predefined time elapses, the system reverts to waiting for interrupts for new incoming packets.

This hybrid mechanism is widely used in various high-performance scenarios. In storage systems, for example, an interrupt might signal the CPU that a disk operation is complete, after which polling can handle the transfer of subsequent data blocks, avoiding the cost of multiple interrupts for a large file transfer. Similarly, in USB devices, interrupts are used for infrequent data transfers, such as mouse or keyboard inputs, while bulk transfers—like those from USB storage or cameras—may rely on polling to improve throughput. Graphics processing units (GPUs) also benefit from this approach, where interrupts notify the CPU of task completion, and polling is used to manage the pipeline of subsequent rendering operations. Another notable application is in high-performance networking, such as in the Data Plane Development Kit (DPDK), where polling is primarily used to handle packets for maximum throughput, while interrupts manage control events like link status changes.

## 1.2. HDD, SSD

### 1.2.1. HDD

<p align="center">
    <img src="imgs/18_1.PNG" width="50%" />
</p>

For example, 7200RPM HDD, avg seek time 5ms, time for rotation is:

$$60000 (ms/min) / 7200(rev/min) = 8.3(ms/rev)$$

So avg rotation delay is about 4ms. (half rotation)

Transfer rate of $50MB/s$, sector size(block size) $4KB$ have

$$4096(B)/(50×10^6(B/s)) = 0.082 (ms/sector)$$

- Reading block from random place on disk has:
  - seek time 5ms + rotation delay 4ms + transfer time 0.082ms = 9.082ms
  - read throughput is $4096B/9.082ms=451KB/s$
- Read block from random place in same cylinder:
  - rotation delay 4ms + transfer time 0.082ms = 4.082ms
  - read throughput is $4096B/4.082ms=1.03MB/s$
- Read next block on same track:
  - read throughput is $4096B/0.082ms=50MB/s$

So, key to using disk effectively (especially for file systems) is to minimize seek and
rotational delays.

### 1.2.2. SSD

<p align="center">
    <img src="imgs/18_2.png" width="60%" />
    <img src="imgs/18_3.png" width="30%" />
</p>

NAND flash cannot overwrite existing data directly because of how it stores information as electrical charges within cells. Instead, the data must first be erased before new data can be written. This is tied to the way NAND memory is structured: data is organized into pages (the smallest writable unit, typically 4KB-16KB), while erasure occurs in larger units called blocks (typically 128KB-4MB). If a page within a block already contains data, the entire block must be erased before the page can be rewritten.

This limitation is rooted in the physical constraints of NAND cells. Data is stored in floating gates as electrical charges, and overwriting these charges directly is not feasible because of interference between adjacent cells. Erasing a block resets all the cells within it, removing the stored charges and preparing them to accept new data. Although this erasure-before-write process is fundamental to NAND flash memory, modern SSDs employ a variety of techniques to mitigate its impact on performance.

**Flash Translation Layer (FTL)** and **Copy-on-Write (CoW)** are two key techniques used in managing SSDs to optimize performance, reliability, and longevity. Together, these mechanisms address the unique characteristics and limitations of NAND flash memory, enabling faster and more efficient operation.

**Flash Translation Layer (FTL)** is a critical abstraction layer in SSDs that maps logical block addresses (LBAs) from the operating system to physical addresses on the NAND flash memory. This mapping is necessary because NAND flash memory cannot directly overwrite data due to its erase-before-write limitation. FTL manages this complexity efficiently, allowing the SSD to present itself to the operating system like a traditional block storage device while internally optimizing data placement and retrieval.

- Wear Leveling:
  - NAND cells have a finite number of program/erase (P/E) cycles. FTL ensures that write operations are evenly distributed across the memory to prevent some cells from wearing out prematurely. This increases the lifespan of the SSD while avoiding performance bottlenecks from overused cells.
- Garbage Collection:
  - FTL handles invalidated data by consolidating valid data from partially used blocks into new blocks, freeing up space for future writes. This process minimizes the number of erase cycles needed, ensuring smoother and faster writes over time.
- Write Amplification Reduction:
  - By intelligently managing how data is written and erased, FTL minimizes write amplification—a phenomenon where the amount of actual data written to NAND is greater than the data intended to be written. Lower write amplification results in faster performance and longer SSD lifespan.
- Mapping Tables:
  - FTL uses mapping tables to redirect writes to free pages instead of overwriting existing data. This redirection eliminates the need to erase blocks immediately and allows SSDs to write new data faster, as the erasure can be deferred to a background process.

**Copy-on-Write (CoW)** is a data management technique that avoids overwriting existing data. Instead of modifying data in place, CoW creates a new copy of the data with the changes applied, leaving the original data intact. Once the new data is successfully written, metadata is updated to point to the new version, and the old version may eventually be discarded during garbage collection.

- Efficient Writes:
  - By writing new data to free pages instead of erasing and rewriting old data, CoW eliminates the overhead associated with the erase-before-write process. This enables faster write operations, particularly for workloads with frequent updates.
- Crash Consistency:
  - CoW ensures data integrity during power failures or crashes. Since the original data remains unmodified until the new data is fully written, there is no risk of corruption from incomplete writes. This is particularly important for databases and filesystems.
- Better Utilization of Parallelism:
  - Modern SSDs can perform multiple write operations in parallel across different NAND dies. CoW leverages this capability by writing to available free pages simultaneously, improving throughput for workloads with high write demands.
- Simplified Garbage Collection:
  - CoW naturally aligns with garbage collection by creating new versions of data, leaving old data to be invalidated and eventually cleaned up. This reduces the complexity of managing data blocks and improves garbage collection efficiency.

## 1.3. Queueing Theory

- Latency $L$
- Request Rate $\lambda$
- Service Rate $\mu$
- Utilization $\rho=\frac{\lambda}{\mu_{max}}$
- Queueing deley $d$
- Operation time $t$

### 1.3.1. Simple, Deterministic World

Events arrive exactly after $T_A$ time.

<p align="center">
    <img src="imgs/19_1.png" width="60%" />
</p>


<p align="center">
    <img src="imgs/19_2.png" width="50%" />
</p>


### 1.3.2. Random Events

Events arrive with average rate $1/T_A$ but is burstable. Every events are independent to each other. This is poisson point process.

<p align="center">
    <img src="imgs/19_3.png" width="60%" />
</p>

<p align="center">
    <img src="imgs/19_4.png" width="50%" />
</p>

Using Little's Law,

$L_Q = \lambda T_Q = \frac{\rho}{T_S} T_Q = \frac{\rho^2}{1-\rho}$,

where $L_Q$ is the average length of the queue.

So if utilization goes to 1, i.e. arrival rate becomes similar to service rate, the average length of the queue becomes infinite, resulting queueing time becoming infinite.

<p align="center">
    <img src="imgs/19_5.png" width="60%" />
</p>

### 1.3.3. We need to make disk performance more faster

To enhance disk performance, it is crucial to optimize for specific workload patterns and leverage system-level strategies effectively. For instance, performance can be improved during **large sequential reads** by optimizing access patterns to minimize seek times and maximize throughput. Similarly, when the workload is substantial, techniques like **reordering queues** allow multiple tasks to be executed efficiently by piggybacking operations, reducing the overall I/O latency. *During periods of low activity, slight inefficiencies may be tolerable*, as the system can prioritize responsiveness over optimization. Bursts of activity, however, present both a challenge and an opportunity—by strategically handling these bursts, such as caching frequently accessed data or prefetching likely-needed resources, the system can mitigate delays. One potential optimization could involve trading storage space for speed, such as using larger blocks or dedicating space for faster access methods. Other advanced techniques include reducing overhead with user-level drivers to bypass kernel bottlenecks and overlapping I/O operations with other productive tasks to make better use of idle CPU cycles. Together, these strategies aim to balance speed, efficiency, and resource utilization.

#### reordering queues

<p align="center">
    <img src="imgs/19_6.png" width="60%" />
    <img src="imgs/19_7.png" width="60%" />
</p>

# 2. FileSystem Case Study

A file system is a critical layer of the operating system designed to abstract and enhance the raw block interface of storage devices, such as disks, into a user-friendly structure of files and directories. At its core, the file system manages the mapping of human-readable file names to blocks of data on the disk, enabling users to interact with their data efficiently and intuitively. Key functionalities of a file system include **naming**, which allows users to locate files by their names rather than block numbers, and **organization**, which ensures that files are properly mapped to disk blocks. It also provides **protection** mechanisms to enforce access controls and **reliability** features to safeguard data against crashes and hardware failures, ensuring durability and accessibility of stored information.

<p align="center">
    <img src="imgs/20_1.png" width="50%" />
</p>

From a user’s perspective, the file system represents durable data structures that store information persistently, regardless of system reboots or interruptions. However, from the operating system’s viewpoint, it manages a collection of bytes, treating files as a sequential arrangement of these bytes without concern for their internal structure. Internally, the file system deals with collections of blocks, where a **block** is the logical transfer unit, often larger than the physical sector size of the disk. For example, in UNIX systems, blocks are typically 4KB in size. This layered abstraction separates user-level operations from the underlying physical management, enabling diverse applications to store and retrieve complex data structures seamlessly.


Disk management is a foundational aspect of the file system. The disk itself is viewed as a linear array of **sectors**, which can be identified using logical block addressing (LBA). Each sector has a unique integer address, and the disk controller translates these addresses into their corresponding physical positions, abstracting the complexity of the disk’s physical geometry. The file system must maintain various metadata structures to track free disk blocks, map file data to blocks, and organize files within directories. These responsibilities ensure efficient space utilization, quick access to files, and robust file organization.

To achieve its objectives, the file system relies on specific data structures stored on the disk. Unlike memory-resident structures, **disk-based data structures must be optimized for block-based access**, as individual words cannot be read or written efficiently. Sequential access patterns are preferred to minimize latency and maximize throughput. **Durability** is another critical design goal, ensuring that the file system remains in a consistent state even after unexpected shutdowns or crashes. Though achieving perfect durability is challenging, modern file systems incorporate techniques like **journaling** and **transactional updates** to minimize the risk of data corruption and maintain operational integrity.

<p align="center">
    <img src="imgs/20_2.png" width="45%" />
</p>

When a program executes an `open(file)` system call, the file system begins a process known as **name resolution** to translate the provided file path into a file number or inode, which serves as a unique identifier for the file. This process involves traversing the directory structure, which is itself a file containing mappings of `<file_name, file_number>` pairs, known as directory entries. Starting from the root directory or the current working directory (for relative paths), the system sequentially reads the directory entries at each level of the path hierarchy until it locates the target file. For example, resolving `/my/book/count` involves reading the root directory’s data to locate `my`, then reading the `my` directory to find `book`, and so on until the final file, `count`, is located.

Once the file is located, the file system retrieves its **inode** from the disk. The inode contains metadata about the file, including its type, size, permissions, and pointers to the blocks where the file’s data resides. This inode is then loaded into an in-memory data structure, the system-wide open file table, which maintains an entry for every opened file. Notably, the table contains only one entry per file, regardless of how many processes have opened it, ensuring efficient management of shared resources. The `open()` call returns a file handle to the program, which is essentially an index to this table, allowing subsequent `read()` or `write()` operations to interact directly with the in-memory inode and bypass the need for repeated disk lookups.

<p align="center">
    <img src="imgs/20_3.png" width="60%" />
</p>

The process of directory traversal and inode loading is designed to be efficient but can involve multiple disk accesses, especially for deep directory hierarchies. To mitigate this, modern file systems often cache directory structures and inodes in memory, significantly reducing the need for disk I/O during repeated operations. Additionally, directories are treated as specialized files, and access to their raw bytes is restricted by the operating system. Instead, functions like `readdir()` provide controlled access to directory contents, ensuring the integrity of the file system structure and preventing unauthorized modifications to directory data. This abstraction maintains a clear boundary between user processes and the internal mechanics of the file system, enhancing both reliability and security.

## 2.1. FAT: File Allocation Table

File Allocation Table (FAT) is a file system developed for personal computers and was the default filesystem for the MS-DOS and Windows 9x operating systems. Originally developed in 1977 for use on floppy disks, it was adapted for use on hard disks and other devices. The increase in disk drive capacity over time drove modifications to the design that resulted in versions: FAT12, FAT16, FAT32, and exFAT. FAT was replaced with NTFS as the default file system on Microsoft operating systems starting with Windows XP. Nevertheless, FAT continues to be commonly used on relatively small capacity solid-state storage technologies such as SD card, MultiMediaCard (MMC) and eMMC because of its compatibility and ease of implementation.

<p align="center">
    <img src="imgs/20_4.png" width="30%" />
</p>

In the FAT file system, reading and writing operations involve interacting with both the directory structure and the File Allocation Table (FAT) to manage file metadata and block mappings.

#### Example: Reading from File 31, Block 2, Offset X

To read from file 31, block 2, offset x, the process begins by locating the file in the directory. Starting at the root directory, the system searches for the directory entry associated with file number 31. Once found, the FAT is indexed using the file number, following its linked list of entries to identify the location of block 2. The system retrieves the block from the disk into memory, then accesses the data at the specified offset x.

#### Example: Writing to File 31, Block 3, Offset Y

For a write operation to file 31, block 3, offset y, the process starts similarly by locating file 31 in the directory and indexing into the FAT using the file number. The FAT is then used to navigate to the chain of blocks associated with the file. If block 3 is not yet allocated, a free block is selected by finding a FAT entry marked as "free." This free block's FAT entry is updated to link to the existing chain of file 31, making it the new block 3. The data for file 31, block 3 is then written to the corresponding block on disk, and the FAT entry for block 3 is updated to reflect the new allocation. This ensures the file's structure remains intact and accessible for future operations.

<p align="center">
    <img src="imgs/20_5.png" width="50%" />
</p>

Directory is implemented as a special file containing a linked list of entries, where each entry stores `<file_name: file_number>` mappings along with file attributes, such as permissions, timestamps, and size. Unlike modern file systems, **FAT stores file attributes directly within the directory entries** rather than associating them with the file's metadata elsewhere. This structure requires a linear search to locate specific entries, making directory access less efficient for large directories. The root directory ("/") is located at a fixed position on the disk, typically starting at block 2, as blocks 0 and 1 are reserved. Additional directories are represented as files themselves, extending the linked list structure, and allowing hierarchical organization within the FAT file system.

i.e. In the original design of the FAT (File Allocation Table) file system, every FAT entry is part of a single linked list structure that spans the entire disk starting from root.

## 2.2. UFS: Unix File System (Berkeley FFS)

#### Inodes in Unix

In Unix file systems, including the Berkeley Fast File System (FFS), the **inode (index node)** is a fundamental data structure that represents a file or directory. Each inode is an entry in an array-like structure on disk, and its index, known as the file number or **inumber**, serves as a unique identifier for the file. Unlike FAT, where metadata is stored in directory entries, inodes store a file's metadata, such as permissions, ownership (user and group), and size, independently of its directory structure. This design enables multiple directory entries, known as hard links, to point to the same inode, providing flexibility in file management and reducing duplication.

<p align="center">
    <img src="imgs/20_6.png" width="40%" />
</p>

The inode employs a **multi-level indexing tree** to locate file data on disk. For small files, direct pointers within the inode itself provide sufficient space for data. For example, with 10 direct pointers and 4KB blocks, a file can span up to 40KB using just direct pointers. For larger files, inodes use **indirect pointers** to reference blocks that contain arrays of block addresses, extending the file's size capacity significantly. This structure can escalate to double indirect and even **triple indirect pointers** for extremely large files, creating an efficient, asymmetric tree that balances performance for both small and large files. 
For instance, with 4KB blocks, since there can be 1024 pointers in each block, a single indirect pointer can address up to 4MB, a double indirect pointer up to 4GB, and a triple indirect pointer up to 4TB, making the system highly scalable.

This hierarchical structure impacts access times depending on the file size and the block being accessed. For example, accessing block #5 in a file may require a single read operation since it is covered by a direct pointer. However, accessing block #340 could involve traversing the double indirect pointer, then the indirect pointer, and finally fetching the data block, requiring three disk accesses in total. Despite this additional complexity, the multi-level indexing design of Unix inodes provides a robust and flexible solution for efficiently managing files of varying sizes while maintaining compatibility with modern storage demands.

#### FFS

The Fast File System (FFS), introduced in BSD 4.2 (1984), improved upon the original Unix file system by addressing critical performance and reliability issues. 
While it retained the same inode structure as its predecessor, it increased block sizes from 1KB to 4KB, enhancing throughput by reducing the overhead of metadata and data management. 
A key innovation was the distribution of inodes across the disk rather than storing them in a single, fixed location on the outermost cylinders. 
This change improved locality, reduced seek times, and increased resilience against hardware failures since a head crash would no longer risk the loss of all inodes. 
Additionally, FFS replaced the freelist mechanism for managing free space with a bitmap allocation strategy, making it more efficient and better suited for contiguous file allocation. 
A 10% reserved disk space policy further ensured performance by reducing fragmentation and maintaining space for efficient block allocation.

<p align="center">
    <img src="imgs/20_7.png" width="40%" />
</p>

One of FFS's most notable features was its introduction of **block groups**, which grouped data blocks, inodes, and free space into localized regions on the disk, known as "cylinder groups." 
This layout minimized large seek operations by placing file metadata and data in the same block group whenever possible. 
For instance, a directory and its files would often reside in the same cylinder group, enabling fast directory listings and reducing the cost of accessing small files. 
For larger files, FFS employed a "first-free" allocation policy that prioritized contiguous blocks within the same group, ensuring efficient sequential access. 
This combination of metadata and data locality significantly reduced fragmentation and improved the performance of both small and large file operations.

While FFS offered many advantages, such as efficient storage for files of varying sizes and improved reliability, it also had some drawbacks. 
Tiny files were inefficient due to the combined overhead of allocating both an inode and a full data block, even for a single byte of data. 
Additionally, its encoding was less efficient for files that were mostly contiguous on disk, as it still relied on indirect block mechanisms. 
The requirement to reserve 10–20% of free space was another trade-off to prevent fragmentation and maintain performance. 
Nevertheless, FFS set a new standard for file system design, balancing performance, reliability, and scalability for a wide range of workloads.

#### Ext

The Ext family of file systems (Ext2, Ext3, and Ext4) draws significant inspiration from the Fast File System (FFS) in its design, particularly in its use of block groups to optimize locality and performance. 
Similar to FFS's cylinder groups, Ext2 divides the disk into block groups, each containing its own metadata (like free block and inode bitmaps) and file data, ensuring that data and related metadata are stored close to each other. 
This organization reduces seek times and improves access efficiency, especially for small files. 
The inode structure in Ext2 is also similar to FFS, with multiple direct pointers (12 in Ext2), and indirect, double indirect, and triple indirect pointers for large files, balancing performance across a wide range of file sizes.

Ext3 builds upon Ext2 by adding journaling, which enhances reliability by recording metadata changes in a journal before applying them to the file system. 
This feature provides crash consistency and ensures that the file system can recover quickly from unexpected shutdowns, with minimal performance overhead. 
Ext4, the successor to Ext3, extends these ideas further with features like extent-based allocation, larger file and volume size limits, and improved journaling, making it more robust and scalable for modern workloads. 
Overall, the Ext family combines the locality and efficiency principles of FFS with advanced features tailored for Linux environments.

## 2.3. Windows NTFS: New Technology File System

<p align="center">
    <img src="imgs/21_1.png" width="50%" />
</p>

The **New Technology File System (NTFS)**, used in modern Windows systems, is a robust, high-performance file system designed to support large storage capacities, advanced metadata, and enhanced reliability. 
Unlike older file systems like FAT, NTFS uses the **Master File Table (MFT)**, which acts as a centralized database for file metadata and data. 
Each file or directory is represented by an entry in the MFT, which is a fixed-size record (typically 1 KB) containing `attribute:value` pairs. 
These attributes can store metadata (e.g., permissions, timestamps) and small file data directly in the MFT entry itself, making access to small files extremely fast. 
For larger files, the MFT entry holds pointers to **extents**, which are variable-length contiguous regions on disk, optimizing space usage and performance.

<p align="center">
    <img src="imgs/21_2.png" width="49%" />
    <img src="imgs/21_3.png" width="45%" />
    <img src="imgs/21_4.png" width="45%" />
</p>

NTFS's use of **extents** provides significant flexibility in file storage. 
Unlike fixed-size blocks in older systems, extents allow variable-length contiguous allocations, reducing fragmentation and improving I/O efficiency. 
When a file is created, NTFS can use hints to allocate appropriate space, ensuring efficient storage for both small and large files. 
Large files that cannot fit within a single MFT entry can use additional entries linked via a tree-like structure, allowing the system to scale gracefully. 
This design shares similarities with extent-based allocation in modern Linux file systems like ext4. 
Additionally, NTFS supports journaling, which logs metadata changes before applying them, ensuring reliability and fast recovery in the event of a system crash.

Directories in NTFS are implemented as B-trees, providing efficient searching and organization for large directories. 
Each directory record is an entry in the MFT, just like files, but contains index information instead of data. 
Small directories are stored entirely within their MFT entry, while large directories use pointers to external clusters to store overflow entries. 
This hierarchical structure ensures fast access to directory contents, even as the number of files grows significantly. 
NTFS also supports hard links by allowing multiple file name attributes in a single MFT entry, enabling a single file to appear in multiple directories without duplication.

The MFT also includes special features to enhance reliability. 
For example, NTFS reserves the first 16 records in the MFT for system metadata, including a mirror of the MFT that allows recovery in case of corruption. 
Combined with its advanced storage, journaling, and metadata capabilities, NTFS provides a versatile and reliable file system suitable for modern computing needs. 
This structure ensures efficient file access, reduced fragmentation, and the ability to handle complex file operations seamlessly.

# 3. FileSystem Features

## 2.1. mmap

Memory-mapped files provide an efficient mechanism for accessing file data by mapping a file directly into a process's virtual address space. 
Instead of using traditional I/O operations, which involve explicit system calls and multiple memory copies, memory mapping allows the operating system to associate file-backed regions of virtual memory with the file on disk. 
When a process accesses the mapped memory region, the operating system's page fault handler transparently loads the corresponding data into physical memory on demand. 
This avoids pulling the entire file into memory upfront, significantly improving performance and resource utilization for large files. 
Writing to the mapped memory updates the file implicitly, with changes flushed back to disk as part of normal memory management operations.

The `mmap()` system call is used to create these mappings, either by specifying a particular region in the process's address space or letting the operating system choose an available one. 
This mechanism is particularly useful for sharing data between processes, as multiple processes can map the same file and share updates without additional synchronization mechanisms. 
Executable files also benefit from memory mapping, as they are paged into memory incrementally during execution, leveraging the same page fault and demand-paging mechanisms. 
By backing page table entries with file data on disk and loading data into physical memory only when accessed, `mmap()` optimizes I/O operations and provides a powerful tool for file manipulation and inter-process communication.

```
mmap(2)                                              System Calls Manual                                             mmap(2)

NAME
       mmap, munmap - map or unmap files or devices into memory

LIBRARY
       Standard C library (libc, -lc)

SYNOPSIS
       #include <sys/mman.h>

       void *mmap(void addr[.length], size_t length, int prot, int flags,
                  int fd, off_t offset);
       int munmap(void addr[.length], size_t length);

       See NOTES for information on feature test macro requirements.

DESCRIPTION
       mmap()  creates  a new mapping in the virtual address space of the calling process.  The starting address for the new
       mapping is specified in addr.  The length argument specifies the length of the mapping (which must  be  greater  than
       0).
```

#### Example

```c
#include <sys/mman.h> /* also stdio.h, stdlib.h, string.h, fcntl.h, unistd.h */

int something = 162;
int main (int argc, char *argv[]) {
  int myfd;
  char *mfile;
  printf("Data at: %16lx\n", (long unsigned int) &something);
  printf("Heap at : %16lx\n", (long unsigned int) malloc(1));
  printf("Stack at: %16lx\n", (long unsigned int) &mfile);

  /* Open the file */
  myfd = open(argv[1], O_RDWR | O_CREAT);
  if (myfd < 0) { perror("open failed!");exit(1); }

  /* map the file */
  mfile = mmap(0, 10000, PROT_READ|PROT_WRITE, MAP_FILE|MAP_SHARED, myfd, 0);
  if (mfile == MAP_FAILED) {perror("mmap failed"); exit(1);}

  printf("mmap at : %16lx\n", (long unsigned int) mfile);

  puts(mfile);
  strcpy(mfile+20,"Let's write over it");
  close(myfd);
  return 0;
}
```
```
$ ./mmap test
Data at: 105d63058
Heap at : 7f8a33c04b70
Stack at: 7fff59e9db10
mmap at : 105d97000
This is line one
This is line two
This is line three
This is line four

$ cat test
This is line one
ThiLet's write over its line three
This is line four
```